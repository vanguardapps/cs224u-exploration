[
  "SPECIAL_TOKENS_ATTRIBUTES",
  "__annotations__",
  "__call__",
  "__class__",
  "__delattr__",
  "__dict__",
  "__dir__",
  "__doc__",
  "__eq__",
  "__format__",
  "__ge__",
  "__getattribute__",
  "__gt__",
  "__hash__",
  "__init__",
  "__init_subclass__",
  "__le__",
  "__len__",
  "__lt__",
  "__module__",
  "__ne__",
  "__new__",
  "__reduce__",
  "__reduce_ex__",
  "__repr__",
  "__setattr__",
  "__sizeof__",
  "__str__",
  "__subclasshook__",
  "__weakref__",
  "_add_tokens",
  "_additional_special_tokens",
  "_auto_class",
  "_batch_encode_plus",
  "_bos_token",
  "_build_translation_inputs",
  "_call_one",
  "_cls_token",
  "_compile_jinja_template",
  "_convert_encoding",
  "_convert_id_to_token",
  "_convert_token_to_id_with_added_voc",
  "_create_repo",
  "_decode",
  "_decode_use_source_tokenizer",
  "_encode_plus",
  "_eos_token",
  "_eventual_warn_about_too_long_sequence",
  "_eventually_correct_t5_max_length",
  "_from_pretrained",
  "_get_files_timestamps",
  "_get_padding_truncation_strategies",
  "_in_target_context_manager",
  "_mask_token",
  "_pad",
  "_pad_token",
  "_pad_token_type_id",
  "_processor_class",
  "_save_pretrained",
  "_sep_token",
  "_set_processor_class",
  "_src_lang",
  "_switch_to_input_mode",
  "_switch_to_target_mode",
  "_tokenizer",
  "_unk_token",
  "_upload_modified_files",
  "add_special_tokens",
  "add_tokens",
  "added_tokens_decoder",
  "added_tokens_encoder",
  "additional_special_tokens",
  "additional_special_tokens_ids",
  "all_special_ids",
  "all_special_tokens",
  "all_special_tokens_extended",
  "apply_chat_template",
  "as_target_tokenizer",
  "backend_tokenizer",
  "batch_decode",
  "batch_encode_plus",
  "bos_token",
  "bos_token_id",
  "build_inputs_with_special_tokens",
  "can_save_slow_tokenizer",
  "chat_template",
  "clean_up_tokenization",
  "clean_up_tokenization_spaces",
  "cls_token",
  "cls_token_id",
  "convert_added_tokens",
  "convert_ids_to_tokens",
  "convert_tokens_to_ids",
  "convert_tokens_to_string",
  "create_token_type_ids_from_sequences",
  "cur_lang_code",
  "decode",
  "decoder",
  "default_chat_template",
  "deprecation_warnings",
  "encode",
  "encode_plus",
  "eos_token",
  "eos_token_id",
  "from_pretrained",
  "get_added_vocab",
  "get_special_tokens_mask",
  "get_vocab",
  "init_inputs",
  "init_kwargs",
  "is_fast",
  "lang_code_to_id",
  "legacy_behaviour",
  "mask_token",
  "mask_token_id",
  "max_len_sentences_pair",
  "max_len_single_sentence",
  "max_model_input_sizes",
  "model_input_names",
  "model_max_length",
  "name_or_path",
  "num_special_tokens_to_add",
  "pad",
  "pad_token",
  "pad_token_id",
  "pad_token_type_id",
  "padding_side",
  "prefix_tokens",
  "prepare_for_model",
  "prepare_seq2seq_batch",
  "pretrained_init_configuration",
  "pretrained_vocab_files_map",
  "push_to_hub",
  "register_for_auto_class",
  "sanitize_special_tokens",
  "save_pretrained",
  "save_vocabulary",
  "sep_token",
  "sep_token_id",
  "set_src_lang_special_tokens",
  "set_tgt_lang_special_tokens",
  "set_truncation_and_padding",
  "slow_tokenizer_class",
  "special_tokens_map",
  "special_tokens_map_extended",
  "split_special_tokens",
  "src_lang",
  "suffix_tokens",
  "tgt_lang",
  "tokenize",
  "train_new_from_iterator",
  "truncate_sequences",
  "truncation_side",
  "unk_token",
  "unk_token_id",
  "verbose",
  "vocab",
  "vocab_file",
  "vocab_files_names",
  "vocab_size"
]
